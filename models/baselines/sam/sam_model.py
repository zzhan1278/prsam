#!/usr/bin/env python3
"""
SAM Fine-tuned Model for Medical Image Segmentation
åŸºäºSegment Anything Modelçš„åŒ»å­¦å›¾åƒåˆ†å‰²å¾®è°ƒç‰ˆæœ¬
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from segment_anything import sam_model_registry, SamPredictor
from segment_anything.modeling import Sam
import os

class SAMFineTuned(nn.Module):
    """
    SAM Fine-tuned for medical image segmentation
    å†»ç»“å›¾åƒç¼–ç å™¨ï¼Œåªå¾®è°ƒmaskè§£ç å™¨
    """
    
    def __init__(self, model_type="vit_b", checkpoint_path=None, freeze_encoder=True):
        super(SAMFineTuned, self).__init__()
        
        self.model_type = model_type
        self.freeze_encoder = freeze_encoder
        
        # é»˜è®¤checkpointè·¯å¾„
        if checkpoint_path is None:
            checkpoint_path = self._get_default_checkpoint_path(model_type)
        
        # åŠ è½½SAMæ¨¡å‹
        if os.path.exists(checkpoint_path):
            self.sam = sam_model_registry[model_type](checkpoint=checkpoint_path)
            print(f"âœ… åŠ è½½SAMæ¨¡å‹: {checkpoint_path}")
        else:
            print(f"âš ï¸ SAM checkpointä¸å­˜åœ¨: {checkpoint_path}")
            print("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„SAMæ¨¡å‹")
            self.sam = sam_model_registry[model_type]()
        
        # å†»ç»“å›¾åƒç¼–ç å™¨
        if freeze_encoder:
            for param in self.sam.image_encoder.parameters():
                param.requires_grad = False
            print("ğŸ”’ SAMå›¾åƒç¼–ç å™¨å·²å†»ç»“")
        
        # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        self.sam.train()
        
        # æ·»åŠ é€‚é…å±‚ç”¨äºåŒ»å­¦å›¾åƒ
        self.medical_adapter = nn.Sequential(
            nn.Conv2d(1, 3, kernel_size=1),  # å•é€šé“è½¬RGB
            nn.BatchNorm2d(3),
            nn.ReLU(inplace=True)
        )
        
        # è¾“å‡ºé€‚é…å±‚
        self.output_adapter = nn.Sequential(
            nn.Conv2d(1, 1, kernel_size=3, padding=1),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        
    def _get_default_checkpoint_path(self, model_type):
        """è·å–é»˜è®¤çš„checkpointè·¯å¾„"""
        checkpoint_paths = {
            "vit_b": "checkpoints/sam_vit_b_01ec64.pth",
            "vit_l": "checkpoints/sam_vit_l_0b3195.pth", 
            "vit_h": "checkpoints/sam_vit_h_4b8939.pth"
        }
        return checkpoint_paths.get(model_type, "checkpoints/sam_vit_b_01ec64.pth")
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­
        Args:
            x: è¾“å…¥å›¾åƒ [B, 1, H, W]
        Returns:
            mask_logits: åˆ†å‰²mask logits [B, 1, H, W]
        """
        batch_size, _, height, width = x.shape
        
        # è½¬æ¢ä¸ºRGBæ ¼å¼
        x_rgb = self.medical_adapter(x)
        
        # è°ƒæ•´åˆ°SAMæœŸæœ›çš„è¾“å…¥å°ºå¯¸ (1024x1024)
        x_resized = F.interpolate(x_rgb, size=(1024, 1024), mode='bilinear', align_corners=False)
        
        # SAMå›¾åƒç¼–ç 
        with torch.set_grad_enabled(not self.freeze_encoder):
            image_embeddings = self.sam.image_encoder(x_resized)
        
        # ç”Ÿæˆé»˜è®¤çš„prompt embeddings (æ— ç‰¹å®šprompt)
        sparse_embeddings = torch.empty((batch_size, 0, 256), device=x.device, dtype=x.dtype)
        dense_embeddings = self.sam.prompt_encoder.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(
            batch_size, -1, self.sam.prompt_encoder.image_embedding_size[0], 
            self.sam.prompt_encoder.image_embedding_size[1]
        )
        
        # SAM maskè§£ç 
        low_res_masks, iou_predictions = self.sam.mask_decoder(
            image_embeddings=image_embeddings,
            image_pe=self.sam.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
        )
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸
        masks = F.interpolate(
            low_res_masks,
            size=(height, width),
            mode='bilinear',
            align_corners=False
        )
        
        # è¾“å‡ºé€‚é…
        output = self.output_adapter(masks)
        
        return output
    
    def get_image_embeddings(self, x):
        """è·å–å›¾åƒembeddingsï¼Œç”¨äºprompt-basedæ¨ç†"""
        x_rgb = self.medical_adapter(x)
        x_resized = F.interpolate(x_rgb, size=(1024, 1024), mode='bilinear', align_corners=False)
        
        with torch.no_grad():
            image_embeddings = self.sam.image_encoder(x_resized)
        
        return image_embeddings
    
    def predict_with_points(self, x, points, labels):
        """
        ä½¿ç”¨ç‚¹promptè¿›è¡Œé¢„æµ‹
        Args:
            x: è¾“å…¥å›¾åƒ [B, 1, H, W]
            points: ç‚¹åæ ‡ [B, N, 2]
            labels: ç‚¹æ ‡ç­¾ [B, N] (1ä¸ºå‰æ™¯ï¼Œ0ä¸ºèƒŒæ™¯)
        """
        batch_size, _, height, width = x.shape
        
        # è·å–å›¾åƒembeddings
        image_embeddings = self.get_image_embeddings(x)
        
        # è°ƒæ•´ç‚¹åæ ‡åˆ°1024å°ºå¯¸
        points_1024 = points.clone()
        points_1024[:, :, 0] = points_1024[:, :, 0] * 1024 / width
        points_1024[:, :, 1] = points_1024[:, :, 1] * 1024 / height
        
        # ç¼–ç prompt
        sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(
            points=(points_1024, labels),
            boxes=None,
            masks=None,
        )
        
        # è§£ç mask
        low_res_masks, iou_predictions = self.sam.mask_decoder(
            image_embeddings=image_embeddings,
            image_pe=self.sam.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
        )
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸
        masks = F.interpolate(
            low_res_masks,
            size=(height, width),
            mode='bilinear',
            align_corners=False
        )
        
        return masks, iou_predictions


class SAMWithPrompts(SAMFineTuned):
    """
    SAM with automatic prompt generation from ground truth
    è‡ªåŠ¨ä»ground truthç”Ÿæˆpromptçš„SAMç‰ˆæœ¬
    """
    
    def __init__(self, model_type="vit_b", checkpoint_path=None, freeze_encoder=True, 
                 num_points=5, use_bbox=False):
        super().__init__(model_type, checkpoint_path, freeze_encoder)
        self.num_points = num_points
        self.use_bbox = use_bbox
    
    def extract_points_from_mask(self, mask, num_points=5):
        """
        ä»maskä¸­æå–å‰æ™¯å’ŒèƒŒæ™¯ç‚¹
        Args:
            mask: ground truth mask [B, 1, H, W]
            num_points: æå–çš„ç‚¹æ•°
        Returns:
            points: ç‚¹åæ ‡ [B, num_points*2, 2]
            labels: ç‚¹æ ‡ç­¾ [B, num_points*2]
        """
        batch_size, _, height, width = mask.shape
        points_list = []
        labels_list = []
        
        for b in range(batch_size):
            mask_b = mask[b, 0].cpu().numpy()
            
            # å‰æ™¯ç‚¹
            fg_coords = np.where(mask_b > 0.5)
            if len(fg_coords[0]) > 0:
                fg_indices = np.random.choice(len(fg_coords[0]), 
                                            min(num_points, len(fg_coords[0])), 
                                            replace=False)
                fg_points = np.stack([fg_coords[1][fg_indices], fg_coords[0][fg_indices]], axis=1)
            else:
                fg_points = np.array([[width//2, height//2]])  # é»˜è®¤ä¸­å¿ƒç‚¹
            
            # èƒŒæ™¯ç‚¹
            bg_coords = np.where(mask_b <= 0.5)
            if len(bg_coords[0]) > 0:
                bg_indices = np.random.choice(len(bg_coords[0]), 
                                            min(num_points, len(bg_coords[0])), 
                                            replace=False)
                bg_points = np.stack([bg_coords[1][bg_indices], bg_coords[0][bg_indices]], axis=1)
            else:
                bg_points = np.array([[0, 0]])  # é»˜è®¤è§’è½ç‚¹
            
            # åˆå¹¶ç‚¹
            points = np.concatenate([fg_points, bg_points], axis=0)
            labels = np.concatenate([np.ones(len(fg_points)), np.zeros(len(bg_points))])
            
            points_list.append(torch.from_numpy(points).float())
            labels_list.append(torch.from_numpy(labels).float())
        
        # è½¬æ¢ä¸ºtensor
        max_points = max(len(p) for p in points_list)
        points_tensor = torch.zeros(batch_size, max_points, 2)
        labels_tensor = torch.zeros(batch_size, max_points)
        
        for b, (pts, lbls) in enumerate(zip(points_list, labels_list)):
            points_tensor[b, :len(pts)] = pts
            labels_tensor[b, :len(lbls)] = lbls
        
        return points_tensor.to(mask.device), labels_tensor.to(mask.device)
    
    def forward(self, x, gt_mask=None):
        """
        å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨ground truthç”Ÿæˆprompt
        """
        if gt_mask is not None and self.training:
            # è®­ç»ƒæ—¶ä½¿ç”¨GTç”Ÿæˆprompt
            points, labels = self.extract_points_from_mask(gt_mask, self.num_points)
            masks, iou_pred = self.predict_with_points(x, points, labels)
            return masks
        else:
            # æ¨ç†æ—¶ä½¿ç”¨æ— promptæ¨¡å¼
            return super().forward(x)


def create_sam_model(model_type="vit_b", checkpoint_path=None, freeze_encoder=True, 
                    use_prompts=True, **kwargs):
    """
    åˆ›å»ºSAMæ¨¡å‹çš„å·¥å‚å‡½æ•°
    """
    if use_prompts:
        return SAMWithPrompts(model_type, checkpoint_path, freeze_encoder, **kwargs)
    else:
        return SAMFineTuned(model_type, checkpoint_path, freeze_encoder)


if __name__ == "__main__":
    # æµ‹è¯•SAMæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæ¨¡å‹
    model = SAMFineTuned(model_type="vit_b").to(device)
    
    # æµ‹è¯•è¾“å…¥
    x = torch.randn(2, 1, 256, 256).to(device)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(x)
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"è¾“å‡ºèŒƒå›´: [{output.min():.4f}, {output.max():.4f}]")
    
    print("âœ… SAMæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
