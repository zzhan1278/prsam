#!/usr/bin/env python3
"""
MedSAM Model for Medical Image Segmentation
åŸºäºå®˜æ–¹MedSAMå®ç°ï¼Œä½¿ç”¨çœŸæ­£çš„MedSAMé¢„è®­ç»ƒæƒé‡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os

# å°è¯•å¯¼å…¥MedSAMç›¸å…³æ¨¡å—
try:
    from segment_anything import sam_model_registry
    from segment_anything.modeling import Sam
except ImportError:
    print("Warning: segment_anything not found, using fallback implementation")

class MedSAM(nn.Module):
    """
    MedSAM - Medical Segment Anything Model
    é¢„è®­ç»ƒçš„åŒ»å­¦å›¾åƒSAMï¼Œç›´æ¥ç”¨äºæ¨ç†
    """
    
    def __init__(self, model_type="vit_b", checkpoint_path=None, bbox_prompt=True):
        super(MedSAM, self).__init__()
        
        self.model_type = model_type
        self.bbox_prompt = bbox_prompt
        
        # é»˜è®¤checkpointè·¯å¾„
        if checkpoint_path is None:
            checkpoint_path = self._get_default_checkpoint_path(model_type)
        
        # æŒ‰ç…§MedSAMå®˜æ–¹æ–¹å¼åŠ è½½æ¨¡å‹
        if os.path.exists(checkpoint_path):
            print(f"âœ… æ‰¾åˆ°MedSAMæƒé‡: {checkpoint_path}")
            try:
                # ä½¿ç”¨å®˜æ–¹MedSAMåŠ è½½æ–¹å¼
                sam_model = sam_model_registry[model_type](checkpoint=checkpoint_path)
                print(f"âœ… æˆåŠŸåŠ è½½MedSAMæ¨¡å‹")
            except Exception as e:
                print(f"âš ï¸ MedSAMåŠ è½½å¤±è´¥: {e}")
                print("ğŸ”„ ä½¿ç”¨æ ‡å‡†SAMä½œä¸ºæ›¿ä»£")
                standard_path = self._get_standard_sam_path(model_type)
                if os.path.exists(standard_path):
                    sam_model = sam_model_registry[model_type](checkpoint=standard_path)
                    print(f"âœ… åŠ è½½æ ‡å‡†SAM: {standard_path}")
                else:
                    sam_model = sam_model_registry[model_type]()
                    print("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„SAMæ¨¡å‹")
        else:
            print(f"âš ï¸ MedSAMæƒé‡ä¸å­˜åœ¨: {checkpoint_path}")
            print("ğŸ”„ ä½¿ç”¨æ ‡å‡†SAMä½œä¸ºæ›¿ä»£")
            standard_path = self._get_standard_sam_path(model_type)
            if os.path.exists(standard_path):
                sam_model = sam_model_registry[model_type](checkpoint=standard_path)
                print(f"âœ… åŠ è½½æ ‡å‡†SAM: {standard_path}")
            else:
                sam_model = sam_model_registry[model_type]()
                print("ğŸ”„ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„SAMæ¨¡å‹")
        
        # å­˜å‚¨SAMæ¨¡å‹
        self.sam_model = sam_model
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.sam_model.eval()
        
        # å†»ç»“æ‰€æœ‰å‚æ•°
        for param in self.sam_model.parameters():
            param.requires_grad = False
        
        # åŒ»å­¦å›¾åƒé€‚é…å±‚
        self.medical_adapter = nn.Sequential(
            nn.Conv2d(1, 3, kernel_size=1),  # å•é€šé“è½¬RGB
            nn.BatchNorm2d(3),
            nn.ReLU(inplace=True)
        )
        
        # è¾“å‡ºåå¤„ç†å±‚
        self.output_processor = nn.Sequential(
            nn.Conv2d(1, 1, kernel_size=3, padding=1),
            nn.BatchNorm2d(1),
            nn.Sigmoid()
        )
        
    def _get_default_checkpoint_path(self, model_type):
        """è·å–MedSAMçš„é»˜è®¤checkpointè·¯å¾„"""
        checkpoint_paths = {
            "vit_b": "checkpoints/medsam_vit_b.pth",
            "vit_l": "checkpoints/medsam_vit_l.pth", 
            "vit_h": "checkpoints/medsam_vit_h.pth"
        }
        return checkpoint_paths.get(model_type, "checkpoints/medsam_vit_b.pth")
    
    def _get_standard_sam_path(self, model_type):
        """è·å–æ ‡å‡†SAMçš„checkpointè·¯å¾„ä½œä¸ºå¤‡é€‰"""
        checkpoint_paths = {
            "vit_b": "checkpoints/sam_vit_b_01ec64.pth",
            "vit_l": "checkpoints/sam_vit_l_0b3195.pth", 
            "vit_h": "checkpoints/sam_vit_h_4b8939.pth"
        }
        return checkpoint_paths.get(model_type, "checkpoints/sam_vit_b_01ec64.pth")
    
    def generate_bbox_from_mask(self, mask):
        """
        ä»ground truth maskç”Ÿæˆbounding box
        Args:
            mask: ground truth mask [B, 1, H, W]
        Returns:
            boxes: bounding boxes [B, 4] (x1, y1, x2, y2)
        """
        batch_size, _, height, width = mask.shape
        boxes = []
        
        for b in range(batch_size):
            mask_b = mask[b, 0].cpu().numpy()
            
            # æ‰¾åˆ°å‰æ™¯åŒºåŸŸ
            coords = np.where(mask_b > 0.5)
            
            if len(coords[0]) > 0:
                y_min, y_max = coords[0].min(), coords[0].max()
                x_min, x_max = coords[1].min(), coords[1].max()
                
                # æ·»åŠ ä¸€äº›padding
                padding = 5
                x_min = max(0, x_min - padding)
                y_min = max(0, y_min - padding)
                x_max = min(width - 1, x_max + padding)
                y_max = min(height - 1, y_max + padding)
                
                boxes.append([x_min, y_min, x_max, y_max])
            else:
                # å¦‚æœæ²¡æœ‰å‰æ™¯ï¼Œä½¿ç”¨æ•´ä¸ªå›¾åƒ
                boxes.append([0, 0, width - 1, height - 1])
        
        return torch.tensor(boxes, dtype=torch.float32, device=mask.device)
    
    def forward(self, x, gt_mask=None):
        """
        å‰å‘ä¼ æ’­ - ç›´æ¥ä½¿ç”¨MedSAMæ¨¡å‹
        Args:
            x: è¾“å…¥å›¾åƒ [B, 1, H, W]
            gt_mask: ground truth mask [B, 1, H, W] (å¯é€‰)
        Returns:
            mask_logits: åˆ†å‰²mask logits [B, 1, H, W]
        """
        batch_size, _, height, width = x.shape
        
        # è½¬æ¢ä¸ºRGBæ ¼å¼
        x_rgb = self.medical_adapter(x)
        
        # è°ƒæ•´åˆ°SAMæœŸæœ›çš„è¾“å…¥å°ºå¯¸ (1024x1024)
        x_resized = F.interpolate(x_rgb, size=(1024, 1024), mode='bilinear', align_corners=False)
        
        # SAMå›¾åƒç¼–ç 
        with torch.no_grad():
            image_embeddings = self.sam_model.image_encoder(x_resized)
        
        # ä½¿ç”¨å…¨å›¾ä½œä¸ºbbox prompt
        boxes = torch.tensor([[0, 0, 1023, 1023]], dtype=torch.float32, device=x.device).repeat(batch_size, 1)
        
        # ç¼–ç prompt
        sparse_embeddings, dense_embeddings = self.sam_model.prompt_encoder(
            points=None,
            boxes=boxes,
            masks=None,
        )
        
        # SAM maskè§£ç 
        with torch.no_grad():
            low_res_masks, iou_predictions = self.sam_model.mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=self.sam_model.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=False,
            )
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸
        masks = F.interpolate(
            low_res_masks,
            size=(height, width),
            mode='bilinear',
            align_corners=False
        )
        
        # è¾“å‡ºåå¤„ç†
        output = self.output_processor(masks)
        
        return output
    
    def predict_with_bbox(self, x, boxes):
        """
        ä½¿ç”¨bounding boxè¿›è¡Œé¢„æµ‹
        Args:
            x: è¾“å…¥å›¾åƒ [B, 1, H, W]
            boxes: bounding boxes [B, 4] (x1, y1, x2, y2)
        """
        batch_size, _, height, width = x.shape
        
        # è½¬æ¢ä¸ºRGBå¹¶è°ƒæ•´å°ºå¯¸
        x_rgb = self.medical_adapter(x)
        x_resized = F.interpolate(x_rgb, size=(1024, 1024), mode='bilinear', align_corners=False)
        
        # è·å–å›¾åƒembeddings
        with torch.no_grad():
            image_embeddings = self.sam.image_encoder(x_resized)
        
        # è°ƒæ•´bboxåˆ°1024å°ºå¯¸
        boxes_1024 = boxes.clone()
        boxes_1024[:, [0, 2]] = boxes_1024[:, [0, 2]] * 1024 / width
        boxes_1024[:, [1, 3]] = boxes_1024[:, [1, 3]] * 1024 / height
        
        # ç¼–ç prompt
        sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(
            points=None,
            boxes=boxes_1024,
            masks=None,
        )
        
        # è§£ç mask
        with torch.no_grad():
            low_res_masks, iou_predictions = self.sam.mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=self.sam.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=False,
            )
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸
        masks = F.interpolate(
            low_res_masks,
            size=(height, width),
            mode='bilinear',
            align_corners=False
        )
        
        return masks, iou_predictions
    
    def predict_with_points(self, x, points, labels):
        """
        ä½¿ç”¨ç‚¹promptè¿›è¡Œé¢„æµ‹
        Args:
            x: è¾“å…¥å›¾åƒ [B, 1, H, W]
            points: ç‚¹åæ ‡ [B, N, 2]
            labels: ç‚¹æ ‡ç­¾ [B, N] (1ä¸ºå‰æ™¯ï¼Œ0ä¸ºèƒŒæ™¯)
        """
        batch_size, _, height, width = x.shape
        
        # è½¬æ¢ä¸ºRGBå¹¶è°ƒæ•´å°ºå¯¸
        x_rgb = self.medical_adapter(x)
        x_resized = F.interpolate(x_rgb, size=(1024, 1024), mode='bilinear', align_corners=False)
        
        # è·å–å›¾åƒembeddings
        with torch.no_grad():
            image_embeddings = self.sam.image_encoder(x_resized)
        
        # è°ƒæ•´ç‚¹åæ ‡åˆ°1024å°ºå¯¸
        points_1024 = points.clone()
        points_1024[:, :, 0] = points_1024[:, :, 0] * 1024 / width
        points_1024[:, :, 1] = points_1024[:, :, 1] * 1024 / height
        
        # ç¼–ç prompt
        sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(
            points=(points_1024, labels),
            boxes=None,
            masks=None,
        )
        
        # è§£ç mask
        with torch.no_grad():
            low_res_masks, iou_predictions = self.sam.mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=self.sam.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=False,
            )
        
        # è°ƒæ•´å›åŸå§‹å°ºå¯¸
        masks = F.interpolate(
            low_res_masks,
            size=(height, width),
            mode='bilinear',
            align_corners=False
        )
        
        return masks, iou_predictions


def create_medsam_model(model_type="vit_b", checkpoint_path=None, bbox_prompt=True, **kwargs):
    """
    åˆ›å»ºMedSAMæ¨¡å‹çš„å·¥å‚å‡½æ•°
    """
    return MedSAM(model_type, checkpoint_path, bbox_prompt, **kwargs)


if __name__ == "__main__":
    # æµ‹è¯•MedSAMæ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæ¨¡å‹
    model = MedSAM(model_type="vit_b").to(device)
    
    # æµ‹è¯•è¾“å…¥
    x = torch.randn(2, 1, 256, 256).to(device)
    gt_mask = torch.randint(0, 2, (2, 1, 256, 256)).float().to(device)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(x, gt_mask)
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"è¾“å‡ºèŒƒå›´: [{output.min():.4f}, {output.max():.4f}]")
    
    print("âœ… MedSAMæ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
